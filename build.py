#%%
# IMPORTS

import asyncio
import math
import os
import pickle
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta, timezone
from functools import lru_cache
from itertools import product
from pathlib import Path
from typing import List, Optional

import nest_asyncio
import numpy as np
import pandas as pd
import yaml
import yfinance as yf
from dotenv import find_dotenv, load_dotenv
from ib_async import IB, Contract, Stock
from loguru import logger
from pyprojroot import here
from scipy.stats import norm
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

# Configure logger to suppress debug messages
logger.remove()
logger.add(lambda msg: print(msg, end=''), level="INFO")

#%% 
# Configuration functions

class Timediff:
    def __init__(
        self, td: timedelta, days: int, hours: int, minutes: int, seconds: float
    ):
        self.td = td
        self.days = days
        self.hours = hours
        self.minutes = minutes
        self.seconds = seconds

def load_config(market: str) -> dict:
    """
    Load configuration for a specific market from YAML and environment variables.
    
    Args:
        market: Market name (e.g., 'SNP', 'NSE')
    
    Returns:
        Dictionary with configuration values
    """
    dotenv_path = find_dotenv()
    load_dotenv(dotenv_path=dotenv_path)

    config_path = ROOT / "config" / f"{market.lower()}_config.yml"
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    # Override config with environment variables if they exist
    for key, value in os.environ.items():
        if key in config:
            config[key] = value
    
    return config

def get_ib_connection(market: str = "SNP", account: str = '') -> IB:
    """
    Create and return an IB connection using config settings.
    
    Args:
        market: Market name to load config for (default: 'SNP')
        account: Account code to receive updates for (default: '')
    
    Returns:
        Connected IB instance
    """
    config = load_config(market)
    port = config.get("PORT", 1300)
    client_id = config.get("CID", 10)
    
    ib = IB()
    ib.connect('127.0.0.1', port, clientId=client_id, account=account)
    print(f"Connected to IB on port {port} with client ID {client_id} (market: {market}, account: {account or 'default'})")
    
    return ib

#%%
# CONSTANTS

ROOT = here()
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
SNP_URL = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
WEEKLYS_URL = "http://www.cboe.com/products/weeklys-options/available-weeklys"
ADDITIONAL_SYMBOLS = ["QQQ", "SPY"]

config = load_config("SNP")
MAX_DTE = config.get("MAX_DTE")


#%%
# Utility functions

def delete_pkl_files(files_to_delete):
    for filename in files_to_delete:
        if not filename.endswith('.pkl'):
            filename += '.pkl'
        file_path = ROOT / "data" / filename
        if file_path.exists():
            file_path.unlink()
            print(f"Deleted {file_path}")

def pickle_me(obj, file_path: Path):
    with open(str(file_path), "wb") as handle:
        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)

def get_pickle(path: Path, print_msg: bool = True):
    try:
        with open(path, "rb") as f:
            output = pickle.load(f)
            print(f"Loaded {path}")
            return output
    except FileNotFoundError:
        if print_msg:
            print(f"File not found: {path}")
        return None

def do_i_refresh(my_path: Path, max_days: float) -> bool:
    """
    Decides whether to refresh the unds data or not based on how many days old it is.
    """
    days_old = how_many_days_old(my_path)

    return days_old is None or days_old > max_days

def how_many_days_old(file_path: Path) -> float:
    file_age = get_file_age(file_path=file_path)

    seconds_in_a_day = 86400
    file_age_in_days = (
        file_age.td.total_seconds() / seconds_in_a_day if file_age else None
    )

    return file_age_in_days

def get_file_age(file_path: Path) -> Optional[Timediff]:
    if not file_path.exists():
        logger.info(f"{file_path} file is not found")
        return None

    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
    time_now = datetime.now()
    td = time_now - file_time

    return split_time_difference(td)

def split_time_difference(diff: timedelta) -> Timediff:
    days = diff.days
    hours, remainder = divmod(diff.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    seconds += diff.microseconds / 1e6

    return Timediff(diff, days, hours, minutes, seconds)

def get_dte(date_input):
    """
    Calculate days to expiration from a date string or pandas Series of date strings.
    
    Args:
        date_input (str or pd.Series): Date string(s) in 'YYYYMMDD' format
    
    Returns:
        float, pd.Series, or None: Number of days from option closing time to current time in UTC,
                                  or None if input is not a string or is null
    """
    # Handle None or non-string, non-Series input
    if date_input is None or (not isinstance(date_input, (str, pd.Series))):
        return None
        
    # If input is a pandas Series, apply the function to each element
    if isinstance(date_input, pd.Series):
        return date_input.apply(get_dte)
    
    # Take first 8 characters if string is longer
    date_str = str(date_input)[:8]
    
    # Parse the date
    try:
        year = int(date_str[:4])
        month = int(date_str[4:6])
        day = int(date_str[6:8])
    except (ValueError, IndexError):
        return None
    
    # Create datetime object at option closing time (4 PM market close)
    try:
        expiry_datetime = datetime(year, month, day, 16, 0, 0, tzinfo=timezone.utc)
    except (ValueError, OverflowError):
        return None
    
    # Get current time in UTC
    current_time = datetime.now(timezone.utc)
    
    # Calculate time difference and convert to days
    time_diff = expiry_datetime - current_time
    days_to_expiry = time_diff.total_seconds() / (24 * 3600)
    
    return days_to_expiry

def get_prec(v: float, base: float) -> float:
    try:
        output = round(round((v) / base) * base, -int(math.floor(math.log10(base))))
    except Exception:
        output = None

    return output

def atm_margin(strike, undPrice, dte, vy):
    """
    Calculates the margin for an at-the-money put sale.
    
    Parameters:
    strike (float): The strike price of the put option.
    undPrice (float): The underlying asset price.
    dte (int): The number of days to expiration.
    vy (float): The volatility of the underlying asset.
    
    Returns:
    float: The margin for the put sale.
    """
    
    # Calculate the time to expiration in years
    t = dte / 365
    
    # Calculate the delta of the put option
    d1 = (np.log(undPrice / strike) + (vy**2 / 2) * t) / (vy * np.sqrt(t))
    delta = -norm.cdf(d1)
    
    # Calculate the margin
    margin = strike * 100 * abs(delta)
    
    return margin

#%%
# Cached helper functions
@lru_cache(maxsize=1)
def _fetch_snp_symbols() -> pd.Series:
    """Fetch S&P 500 symbols from Wikipedia (cached)."""
    try:
        headers = {'User-Agent': USER_AGENT}
        snp_table = pd.read_html(
            SNP_URL, 
            header=0, 
            attrs={"id": "constituents"}, 
            flavor='lxml', 
            storage_options=headers
        )[0]
        return snp_table["Symbol"]
    except Exception as e:
        logger.error(f"Failed to retrieve S&P 500 symbols: {e}")
        return pd.Series(dtype=str)

@lru_cache(maxsize=1)
def _fetch_weeklys() -> pd.Series:
    """Fetch weekly options symbols from CBOE (cached)."""
    try:
        return pd.read_html(WEEKLYS_URL)[0].iloc[:, 1]
    except Exception as e:
        logger.error(f"Failed to retrieve weekly options: {e}")
        return pd.Series(dtype=str)
    

async def _async_fetch_weeklies_yf(symbols, look_ahead_days=MAX_DTE):
    """Filter S&P 500 symbols for those with weekly options (non-third-Friday expirations).
    
    Args:
        symbols (pd.Series): S&P 500 ticker symbols (Name: Symbol, dtype: object).
        look_ahead_days (int): Days to check for expirations (default: 45).
    
    Returns:
        pd.Series: Symbols with weekly options or error messages.
    """
    today = datetime.now().date()
    cutoff = today + timedelta(days=look_ahead_days)
    weekly_symbols = []

    def check_symbol(symbol):
        """Synchronous helper to check one symbol's options."""
        try:
            ticker = yf.Ticker(str(symbol))
            exps = ticker.options
            if exps:
                for exp in exps:
                    exp_date = datetime.strptime(exp, "%Y-%m-%d").date()
                    if today < exp_date <= cutoff and not (15 <= exp_date.day <= 21 and exp_date.weekday() == 4):
                        return symbol
            return None
        except Exception as e:
            return f"Error for {symbol}: {str(e)}"

    async def process_symbol(symbol, executor):
        """Run synchronous check_symbol in thread pool."""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(executor, check_symbol, symbol)
        return result

    async def main():
        with ThreadPoolExecutor(max_workers=10) as executor:  # Limit concurrency to avoid rate limits
            tasks = [process_symbol(symbol, executor) for symbol in symbols.values]
            for result in await async_tqdm.gather(*tasks, desc="Checking symbols"):
                if result:  # Skip None results
                    weekly_symbols.append(result)

    await main()
    return pd.Series(weekly_symbols, name="Symbol").sort_values()

#%%
# Main functions
def get_option_symbols(weeklies: bool = True) -> pd.Series:
    """
    Get S&P 500 symbols, optionally filtered for weekly options.
    
    Args:
        weeklies: If True, return only symbols with weekly options (plus QQQ/SPY).
                  If False, return all S&P 500 symbols.
    
    Returns:
        Series of stock symbols
    """
    try:
        snp_symbols = _fetch_snp_symbols()  # Cached network call
        
        # Return all S&P symbols if not filtering for weeklies
        if not weeklies:
            return snp_symbols
        
        # Fetch weekly options and filter
        weeklys_data = asyncio.run(_async_fetch_weeklies_yf(snp_symbols))  # Async call
        if weeklys_data.empty:
            weeklys_data = _fetch_weeklys()  # Cached network call as backup
        filtered = weeklys_data[weeklys_data.isin(snp_symbols) & weeklys_data.str.isalpha()]
        
        # Add additional symbols and return
        return pd.concat([filtered, pd.Series(ADDITIONAL_SYMBOLS)], ignore_index=True)
        
    except Exception as e:
        logger.error(f"Failed to retrieve option symbols: {e}")
        return pd.Series(ADDITIONAL_SYMBOLS if weeklies else [], dtype=str)

async def _qualify_batch(ib: IB, contracts: list, pbar) -> tuple:
    """Async helper to qualify contracts in batch."""
    qualified = []
    failed = []
    
    # Create async tasks for all contracts
    tasks = [ib.qualifyContractsAsync(contract) for contract in contracts]
    
    # Process tasks as they complete
    for i, task in enumerate(asyncio.as_completed(tasks)):
        try:
            result = await task
            contract = contracts[i]
            if result:
                qualified.extend(result)
                pbar.set_postfix_str(f"✓ {contract.symbol}")
            else:
                failed.append(contract.symbol)
                pbar.set_postfix_str(f"✗ {contract.symbol}")
        except Exception as e:
            contract = contracts[i]
            failed.append(contract.symbol)
            pbar.set_postfix_str(f"✗ {contract.symbol}: {str(e)[:30]}")
        
        pbar.update(1)
    
    return qualified, failed

def qualify_stock_contracts(symbols: pd.Series, market: str = "SNP") -> list:
    """
    Qualify stock contracts with Interactive Brokers using async batch processing.
    
    Args:
        symbols: Series of stock symbols to qualify
        market: Market name for config loading (default: 'SNP')
    
    Returns:
        List of qualified Stock contracts
    """
    ib = None
    
    try:
        # Connect to IB using config
        ib = get_ib_connection(market)
        
        # Create stock contracts
        contracts = [Stock(symbol, 'SMART', 'USD') for symbol in symbols]
        
        # Qualify contracts asynchronously with progress bar
        print(f"Qualifying {len(contracts)} contracts asynchronously...")
        
        with async_tqdm(total=len(contracts), desc="Qualifying contracts", unit="contract") as pbar:
            qualified, failed = ib.run(_qualify_batch(ib, contracts, pbar))
        
        print(f"\n✓ Successfully qualified {len(qualified)}/{len(contracts)} contracts")
        if failed:
            print(f"✗ Failed to qualify {len(failed)} symbols: {', '.join(failed[:10])}")
            if len(failed) > 10:
                print(f"  ... and {len(failed) - 10} more")
        
        return qualified
        
    except Exception as e:
        logger.error(f"Failed to qualify contracts: {e}")
        return []
        
    finally:
        # Disconnect from IB
        if ib and ib.isConnected():
            ib.disconnect()
            print("Disconnected from IB\n")

def get_qualified_symbols(weeklies: bool = True, market: str = "SNP", save: bool = True) -> list:
    """
    Get option symbols and qualify them as stock contracts with IB.
    
    Args:
        weeklies: If True, get weekly options symbols; if False, all S&P 500
        market: Market name for config loading (default: 'SNP')
        save: If True, save the symbols to a pickle file at ROOT/'data'/df_symbols
    
    Returns:
        List of qualified Stock contracts
    """
    symbols = get_option_symbols(weeklies=weeklies)
    print(f"Retrieved {len(symbols)} symbols (weeklies={weeklies})")

    contracts = qualify_stock_contracts(symbols, market=market)
    
    if save:
        pickle_me(contracts, file_path=ROOT/'data'/'symbols.pkl')
    
    return contracts

#%%
# Contract Prices

def get_prices(
    contracts: List[Contract], 
    market: str = "SNP",
    max_wait_time: int = 10,
    snapshot: bool = True,
    batch_size: int = 50
) -> pd.DataFrame:
    """
    Get market prices for a list of qualified contracts.
    
    Args:
        contracts: List of qualified Contract objects
        market: Market name for config loading (default: 'SNP')
        max_wait_time: Maximum seconds to wait for each ticker to populate (default: 10)
        snapshot: If True, request snapshot; if False, stream data (default: True)
        batch_size: Number of contracts to request at once (default: 50)
    
    Returns:
        DataFrame with symbol, bid, ask, last, close, volume, and other price data
    """
    ib = None
    
    try:
        # Connect to IB
        ib = get_ib_connection(market)
        
        # Process contracts in batches to avoid ticker limit
        all_price_data = []
        total_batches = (len(contracts) + batch_size - 1) // batch_size
        
        def is_valid_price(price):
            """Check if price is valid: not None, not -1.0, and not NaN"""
            if price is None:
                return False
            try:
                return price != -1.0 and not math.isnan(price)
            except (TypeError, ValueError):
                return False
        
        for batch_num in tqdm(range(total_batches), desc="Fetching prices", unit="batch"):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(contracts))
            batch_contracts = contracts[start_idx:end_idx]
            
            # Request market data for batch
            tickers = []
            for contract in batch_contracts:
                ticker = ib.reqMktData(contract, '', snapshot, False)
                tickers.append(ticker)
            
            # Wait for each ticker to get first price, with max timeout
            start_time = time.time()
            tickers_pending = set(range(len(tickers)))
            
            while tickers_pending and (time.time() - start_time) < max_wait_time:
                ib.sleep(0.1)  # Small sleep to allow data to come in
                
                # Check which tickers have received data
                for idx in list(tickers_pending):
                    ticker = tickers[idx]
                    # Consider ticker filled if it has last, close, bid, or ask
                    if (is_valid_price(ticker.last) or 
                        is_valid_price(ticker.close) or
                        is_valid_price(ticker.bid) or
                        is_valid_price(ticker.ask)):
                        tickers_pending.remove(idx)
            
            # Extract price data
            for ticker in tickers:
                # Check if data is valid (not -1, nan, or None)
                bid = ticker.bid if is_valid_price(ticker.bid) else None
                ask = ticker.ask if is_valid_price(ticker.ask) else None
                last = ticker.last if is_valid_price(ticker.last) else None
                close = ticker.close if is_valid_price(ticker.close) else None
                
                all_price_data.append({
                    'symbol': ticker.contract.symbol,
                    'conId': ticker.contract.conId,
                    'bid': bid,
                    'ask': ask,
                    'last': last,
                    'close': close,
                    'volume': ticker.volume if is_valid_price(ticker.volume) else None,
                    'high': ticker.high if is_valid_price(ticker.high) else None,
                    'low': ticker.low if is_valid_price(ticker.low) else None,
                    'open': ticker.open if is_valid_price(ticker.open) else None,
                    'bidSize': ticker.bidSize,
                    'askSize': ticker.askSize,
                    'lastSize': ticker.lastSize,
                    'halted': ticker.halted,
                    'time': ticker.time
                })
            
            # Cancel market data subscriptions if not snapshot
            if not snapshot:
                for contract in batch_contracts:
                    ib.cancelMktData(contract)
        
        # Convert to DataFrame
        df = pd.DataFrame(all_price_data)
        
        # Report data quality
        valid_prices = df[df['last'].notna() | df['close'].notna()]
        print(f"\n✓ Retrieved prices for {len(df)} contracts")
        print(f"✓ Valid prices: {len(valid_prices)}/{len(df)} ({100*len(valid_prices)/len(df):.1f}%)")
        
        return df
        
    except Exception as e:
        logger.error(f"Failed to get prices: {e}")
        return pd.DataFrame()
        
    finally:
        # Disconnect from IB
        if ib and ib.isConnected():
            ib.disconnect()
            print("Disconnected from IB\n")

def get_prices_snapshot(
    contracts: List[Contract], 
    market: str = "SNP", 
    batch_size: int = 50,
    max_wait_time: int = 10
) -> pd.DataFrame:
    """
    Get a one-time snapshot of prices for qualified contracts.
    
    Args:
        contracts: List of qualified Contract objects
        market: Market name for config loading (default: 'SNP')
        batch_size: Number of contracts per batch (default: 50)
        max_wait_time: Maximum time to wait for prices (default: 10)
    
    Returns:
        DataFrame with price data
    """
    df = get_prices(
        contracts, 
        market=market, 
        max_wait_time=max_wait_time, 
        snapshot=True,
        batch_size=batch_size
    )
    
    df['price'] = df.apply(
        lambda row: 
            (row['bid'] + row['ask']) / 2 if row['bid'] is not None and row['ask'] is not None and row['bid'] > 0 and row['ask'] > 0 
            else row['last'] if row['last'] is not None 
            else row['close'] if row['close'] is not None 
            else None,
        axis=1
    )
    df['price'] = df['price'].apply(lambda x: get_prec(x, 0.01) if x is not None else None)
    
    return df
    
# Volatility and Prices

def get_volatilities_snapshot(
    contracts: List[Contract],
    market: str = "SNP",
    batch_size: int = 50,
    max_wait_time: int = 10,  # Note: max_wait_time is used as sleep_time in async call
) -> pd.DataFrame:
    """
    Get a one-time snapshot of price, implied volatility (IV), and historical volatility (HV)
    for qualified contracts using the asynchronous 'volatilities' function.

    Args:
        contracts: List of qualified Contract objects (can be Stock or other)
        market: Market name for config loading (default: 'SNP')
        batch_size: Number of contracts per batch (default: 50)
        max_wait_time: Max seconds to wait for each ticker in the async call.
                       This is passed as 'sleep_time' (default: 10)

    Returns:
        DataFrame with symbol, price, implied volatility (iv), and historical volatility (hv).
    """
    ib = None
    all_vol_data = []

    try:
        # Connect to IB
        ib = get_ib_connection(market)

        # Process contracts in batches
        total_batches = (len(contracts) + batch_size - 1) // batch_size

        for batch_num in tqdm(range(total_batches), desc="Fetching volatilities", unit="batch"):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(contracts))
            batch_contracts = contracts[start_idx:end_idx]

            # Run the asynchronous volatility fetching function within the IB event loop
            # ib.run is the synchronous way to execute async code with ib_insync
            batch_results = ib.run(
                volatilities(
                    contracts=batch_contracts,
                    ib=ib,
                    sleep_time=max_wait_time, # Pass max_wait_time as sleep_time for async
                    gentick="106, 104" # Standard genticks for IV (106) and HV (104)
                )
            )
            
            # Process and flatten the batch results
            for contract_key, data in batch_results.items():
                if isinstance(contract_key, Contract):
                    symbol = contract_key.symbol
                    conId = contract_key.conId
                else: # Assuming contract_key is the symbol string
                    symbol = contract_key
                    conId = None # conId not readily available if key is string

                all_vol_data.append({
                    'symbol': symbol,
                    'conId': conId,
                    'price': data.get('price'),
                    'iv': data.get('iv'),
                    'hv': data.get('hv')
                })

        # Convert to DataFrame
        df = pd.DataFrame(all_vol_data)

        # Report data quality
        valid_ivs = df[df['iv'].notna()]
        print(f"\n✓ Retrieved data for {len(df)} contracts")
        print(f"✓ Valid Implied Volatilities: {len(valid_ivs)}/{len(df)} ({100*len(valid_ivs)/len(df):.1f}%)")
        
        return df

    except Exception as e:
        # Assuming logger is defined elsewhere
        # logger.error(f"Failed to get volatilities: {e}") 
        print(f"Failed to get volatilities: {e}")
        return pd.DataFrame()

    finally:
        # Disconnect from IB
        if ib and ib.isConnected():
            ib.disconnect()
            print("Disconnected from IB\n")

async def volatilities(
    contracts: list, ib: IB, sleep_time: int = 3, gentick: str = "106, 104"
) -> dict:
    tasks = [
        get_an_iv(item=c, ib=ib, sleep_time=sleep_time, gentick=gentick)
        for c in contracts
    ]

    results = await asyncio.gather(*tasks)

    return {
        k: v for d in results for k, v in d.items()
    }  # Combine results into a single dictionary

async def get_an_iv(
    ib: IB, item: str, sleep_time: int = 3, gentick: str = "106, 104"
) -> dict:
    stock_contract = Stock(item, "SMART", "USD") if isinstance(item, str) else item

    ticker = ib.reqMktData(
        stock_contract, genericTickList=gentick
    )  # Request market data with gentick

    await asyncio.sleep(sleep_time)  # Use asyncio.sleep instead of ib.sleep

    # Check if ticker.impliedVolatility is NaN and wait if true

    if pd.isna(ticker.impliedVolatility):
        await asyncio.sleep(2)

    ib.cancelMktData(stock_contract)

    # Return a dictionary with the symbol, price, implied volatility, and historical volatility
    key = item if isinstance(item, str) else stock_contract

    price = ticker.last if not pd.isna(ticker.last) else ticker.close  # Get last price

    iv = ticker.impliedVolatility  # Get implied volatility from ticker

    hv = ticker.histVolatility  # Get historical volatility from ticker

    return {key: {"price": price, "iv": iv, "hv": hv}}  # Return structured data

#%%
# Option Chains

async def get_an_option_chain(item: Contract, ib: IB, sleep_time: int = 2):
    try:
        chain = await asyncio.wait_for(
            ib.reqSecDefOptParamsAsync(
                underlyingSymbol=item.symbol,
                futFopExchange="",
                underlyingSecType=item.secType,
                underlyingConId=item.conId,
            ),
            timeout=sleep_time,
        )

        if chain:
            chain = chain[-1] if isinstance(chain, list) else chain

        return chain

    except asyncio.TimeoutError:
        logger.error(f"Timeout occurred while getting option chain for {item.symbol}")
        return None

async def chains(contracts: list, ib: IB, sleep_time: int = 2) -> dict:
    tasks = [
        asyncio.create_task(
            get_an_option_chain(item=c, ib=ib, sleep_time=sleep_time), name=c.symbol
        )
        for c in contracts
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    out = {task.get_name(): result for task, result in zip(tasks, results) if not isinstance(result, Exception)}
    return out

def get_option_chains(
    contracts: List[Contract],
    market: str = "SNP",
    batch_size: int = 50,
    max_wait_time: int = 10,
    inter_batch_delay: float = 0.5
) -> pd.DataFrame:
    """
    Get option chain parameters (expiries, strikes) for a list of underlying contracts.
    Processes contracts in batches with a single IB connection, retrying failed symbols once.
    Returns a DataFrame with all expiry and strike combinations.

    Args:
        contracts: List of qualified Contract objects
        market: Market name for config loading (default: 'SNP')
        batch_size: Number of contracts per batch (default: 50)
        max_wait_time: Maximum seconds to wait for each chain request (default: 10)
        inter_batch_delay: Seconds to wait between batches to avoid rate limits (default: 0.5)

    Returns:
        DataFrame with columns: symbol, conId, tradingClass, expiry, strike, dte
    """
    ib = None
    all_chain_data = []
    failed_symbols = []

    try:
        # Connect to IB
        ib = get_ib_connection(market)

        # Process contracts in batches
        total_batches = (len(contracts) + batch_size - 1) // batch_size

        for batch_num in tqdm(range(total_batches), desc="Fetching option chains", unit="batch"):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(contracts))
            batch_contracts = contracts[start_idx:end_idx]

            # Run the asynchronous chain fetching function
            batch_results = ib.run(
                chains(
                    contracts=batch_contracts,
                    ib=ib,
                    sleep_time=max_wait_time
                )
            )

            # Process results and track failed symbols
            for contract in batch_contracts:
                symbol = contract.symbol
                chain = batch_results.get(symbol)

                if chain is None:
                    failed_symbols.append(contract)
                    all_chain_data.append({
                        'symbol': symbol,
                        'conId': contract.conId,
                        'tradingClass': None,
                        'expiries': None,
                        'strikes': None
                    })
                    continue

                all_chain_data.append({
                    'symbol': symbol,
                    'conId': chain.underlyingConId,
                    'tradingClass': chain.tradingClass,
                    'expiries': chain.expirations,
                    'strikes': chain.strikes
                })

            # Add inter-batch delay to avoid rate limiting
            if batch_num < total_batches - 1:
                ib.sleep(inter_batch_delay)

        # Retry failed symbols once
        if failed_symbols:
            print(f"Retrying {len(failed_symbols)} failed symbols...")
            retry_results = ib.run(
                chains(
                    contracts=failed_symbols,
                    ib=ib,
                    sleep_time=max_wait_time
                )
            )

            # Update all_chain_data with retry results
            retry_data = []
            for contract in failed_symbols:
                symbol = contract.symbol
                chain = retry_results.get(symbol)

                if chain is None:
                    retry_data.append({
                        'symbol': symbol,
                        'conId': contract.conId,
                        'tradingClass': None,
                        'expiries': None,
                        'strikes': None
                    })
                else:
                    retry_data.append({
                        'symbol': symbol,
                        'conId': chain.underlyingConId,
                        'tradingClass': chain.tradingClass,
                        'expiries': chain.expirations,
                        'strikes': chain.strikes
                    })

            # Replace failed entries with retry results
            all_chain_data = [d for d in all_chain_data if d['symbol'] not in [c.symbol for c in failed_symbols]]
            all_chain_data.extend(retry_data)

        # Convert to DataFrame
        df = pd.DataFrame(all_chain_data)

        # Report data quality
        valid_chains = df[df['expiries'].notna()]
        print(f"\n✓ Retrieved data for {len(df)} contracts")
        print(f"✓ Valid option chains: {len(valid_chains)}/{len(df)} ({100*len(valid_chains)/len(df):.1f}%)")

        if df.empty:
            return pd.DataFrame()

        # Expand rows for each expiry and strike combination
        expanded_rows = []
        for index, row in df.iterrows():
            if row['expiries'] is None or row['strikes'] is None:
                continue
            for expiry, strike in product(row['expiries'], row['strikes']):
                expanded_rows.append({
                    'symbol': row['symbol'],
                    # 'conId': row['conId'],
                    # 'tradingClass': row['tradingClass'],
                    'expiry': expiry,
                    'strike': strike
                })

        # Create final DataFrame
        df_out = pd.DataFrame(expanded_rows)
        df_out['dte'] = get_dte(df_out['expiry'])

        # # Remove 'conId' column from the final DataFrame
        # df_out = df_out.drop(columns=['conId'], errors='ignore')

        return df_out

    except Exception as e:
        logger.error(f"Failed to get option chains: {e}")
        return pd.DataFrame()

    finally:
        # Disconnect from IB
        if ib and ib.isConnected():
            ib.disconnect()
            print("Disconnected from IB\n")

# Function to calculate ATM margin for each row
def calculate_atm_margin(row, chains_df, target_dte):
    symbol = row['symbol']
    und_price = row['price']  # Use price from vols_df
    iv = row['iv']
    
    if pd.isna(und_price) or pd.isna(iv):
        return None
    
    # Filter chains_df for the current symbol
    symbol_chains = chains_df[chains_df['symbol'] == symbol]
    
    if symbol_chains.empty:
        return None
    
    # Find the DTE closest to target_dte
    symbol_chains['dte_diff'] = abs(symbol_chains['dte'] - target_dte)
    min_dte_diff = symbol_chains['dte_diff'].min()
    closest_dte_rows = symbol_chains[symbol_chains['dte_diff'] == min_dte_diff]
    
    if closest_dte_rows.empty:
        return None
    
    # Select the first row with the closest DTE to ensure a single value
    closest_dte_row = closest_dte_rows.iloc[0]
    dte = closest_dte_row['dte']
    
    # Find the strike closest to the underlying price among the closest DTE rows
    closest_dte_rows['strike_diff'] = abs(closest_dte_rows['strike'] - und_price)
    closest_strike_row = closest_dte_rows.loc[closest_dte_rows['strike_diff'].idxmin()]
    
    strike = closest_strike_row['strike']
    
    # Calculate ATM margin
    margin = atm_margin(strike=strike, undPrice=und_price, dte=dte, vy=iv)
    return margin

def chains_n_unds():
    """
    Processes qualified contracts, option chains, and calculate margins.
    
    Returns:
        Tuple of DataFrames: (df_chains, df_unds)
    """
    sym_path = ROOT / 'data' / 'symbols.pkl'

    # Get qualified contracts
    if do_i_refresh(my_path=sym_path, max_days=1):
        qualified_contracts = get_qualified_symbols(weeklies=True, market="SNP", save=True)
        pickle_me(qualified_contracts, file_path=sym_path)
    else:
        qualified_contracts = get_pickle(path=sym_path)

    # Get option chains for qualified contracts
    chain_path = ROOT / 'data' / 'df_chains.pkl'
    if do_i_refresh(my_path=chain_path, max_days=1):
        df_chains = get_option_chains(qualified_contracts, market="SNP", batch_size=50)
        pickle_me(df_chains, file_path=chain_path)
    else:
        df_chains = get_pickle(path=chain_path)

    # print(df_chains.head(10))

    # Get price with volatilities and margins for qualified contracts
    df_unds = get_volatilities_snapshot(qualified_contracts, market="SNP", batch_size=50)

    # Load configuration to get VIRGIN_DTE
    config = load_config('SNP')
    virgin_dte = float(config.get('VIRGIN_DTE', 30))  # Default to 30 if not specified

    # Apply the ATM margin calculation
    df_unds['margin'] = df_unds.apply(
        lambda row: calculate_atm_margin(row, df_chains, virgin_dte),
        axis=1
    )

    print(df_unds[['symbol', 'iv', 'hv', 'margin', 'price']].head(10))
    pickle_me(df_unds, file_path=ROOT / 'data' / 'df_unds.pkl')

    return df_chains, df_unds


#%% 
# Test functions - Make symbols
if __name__ == "__main__":

    sym_path = ROOT/'data'/'symbols.pkl'

    # Get qualified contracts
    if do_i_refresh(my_path=sym_path, max_days=1):
        qualified_contracts = get_qualified_symbols(weeklies=True, market="SNP", save=True)
        pickle_me(qualified_contracts, file_path=sym_path)
    else:
        qualified_contracts = get_pickle(path=sym_path)

    #%%
    # Get option chains for qualified contracts
    # Note: chains should be run before unds, as unds uses chains for margin calculation

    chain_path = ROOT/'data'/'df_chains.pkl'
    if do_i_refresh(my_path=chain_path, max_days=1):
        df_chains = get_option_chains(qualified_contracts, market="SNP", batch_size=50)
        pickle_me(df_chains, file_path=chain_path)
    else:
        df_chains = get_pickle(path=chain_path)

    print(df_chains.head(10))

    #%%
    # Get price with volatilities and margins for qualified contracts
    df_unds = get_volatilities_snapshot(qualified_contracts, market="SNP", batch_size=50)

    # Load configuration to get VIRGIN_DTE
    config = load_config('SNP')
    virgin_dte = float(config.get('VIRGIN_DTE', 30))  # Default to 30 if not specified

    # Apply the ATM margin calculation
    df_unds['margin'] = df_unds.apply(
        lambda row: calculate_atm_margin(row, df_chains, virgin_dte),
        axis=1
    )

    print(df_unds[['symbol', 'iv', 'hv', 'margin', 'price']].head(10))
    pickle_me(df_unds, file_path=ROOT/'data'/'df_unds.pkl')

